{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import InterpretableEmbeddingBase, TokenReferenceBase\n",
    "from captum.attr import visualization\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from config import MODEL_PATH\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We need to split forward pass into two part:\n",
    "# 1) embeddings computation\n",
    "# 2) classification\n",
    "\n",
    "def compute_bert_outputs(model_bert, embedding_output, attention_mask=None, head_mask=None):\n",
    "    if attention_mask is None:\n",
    "        attention_mask = torch.ones(embedding_output.shape[0], embedding_output.shape[1]).to(embedding_output)\n",
    "\n",
    "    extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    extended_attention_mask = extended_attention_mask.to(\n",
    "        dtype=next(model_bert.parameters()).dtype)  # fp16 compatibility\n",
    "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "    if head_mask is not None:\n",
    "        if head_mask.dim() == 1:\n",
    "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "            head_mask = head_mask.expand(model_bert.config.num_hidden_layers, -1, -1, -1, -1)\n",
    "        elif head_mask.dim() == 2:\n",
    "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
    "        head_mask = head_mask.to(\n",
    "            dtype=next(model_bert.parameters()).dtype)  # switch to fload if need + fp16 compatibility\n",
    "    else:\n",
    "        head_mask = [None] * model_bert.config.num_hidden_layers\n",
    "\n",
    "    encoder_outputs = model_bert.encoder(embedding_output,\n",
    "                                         extended_attention_mask,\n",
    "                                         head_mask=head_mask)\n",
    "    sequence_output = encoder_outputs[0]\n",
    "    pooled_output = model_bert.pooler(sequence_output)\n",
    "    outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
    "                                                  1:]  # add hidden_states and attentions if they are here\n",
    "    return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class BertModelWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(BertModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        outputs = compute_bert_outputs(self.model.bert, embeddings)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.model.dropout(pooled_output)\n",
    "        logits = self.model.classifier(pooled_output)\n",
    "        return torch.softmax(logits, dim=1)[:, 1].unsqueeze(1)\n",
    "\n",
    "\n",
    "bert_model_wrapper = BertModelWrapper(model)\n",
    "ig = IntegratedGradients(bert_model_wrapper)\n",
    "\n",
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "\n",
    "def interpret_sentence(model_wrapper, sentence, label=1):\n",
    "    model_wrapper.eval()\n",
    "    model_wrapper.zero_grad()\n",
    "\n",
    "    input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])\n",
    "    input_embedding = model_wrapper.model.bert.embeddings(input_ids)\n",
    "\n",
    "    # predict\n",
    "    pred = model_wrapper(input_embedding).item()\n",
    "    pred_ind = round(pred)\n",
    "\n",
    "    # compute attributions and approximation delta using integrated gradients\n",
    "    attributions_ig, delta = ig.attribute(input_embedding, n_steps=500, return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', pred_ind, '(', '%.2f' % pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].numpy().tolist())\n",
    "    add_attributions_to_visualizer(attributions_ig, tokens, pred, pred_ind, label, delta, vis_data_records_ig)\n",
    "\n",
    "\n",
    "def add_attributions_to_visualizer(attributions, tokens, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "        attributions,\n",
    "        pred,\n",
    "        pred_ind,\n",
    "        label,\n",
    "        \"label\",\n",
    "        attributions.sum(),\n",
    "        tokens[:len(attributions)],\n",
    "        delta))\n",
    "\n",
    "\n",
    "interpret_sentence(bert_model_wrapper, sentence=\"text to classify\", label=0)\n",
    "visualization.visualize_text(vis_data_records_ig)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}